---
title: "dbt Sharpens Data Science"
author: "Alex Abraham"
date: "2023-08-16"
output: 
  xaringan::moon_reader:
    css: [default, tamu, tamu-fonts]
    self_contained: true
    nature:
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# dbt Resolves Data Chaos, Through Standards

- Data transformation: "a cohesive arc moving data from _source-conformed_ to _business-conformed_ " (`dbt` blog)

- Data have idiosyncrasies, so transformations easily adopt adhoc designs

  - Adhoc designs cost high cognitive load in maintenance, collaboration ...
  - 2x trouble if that design is not your payload ("load that pays")
  - Design standards will cost time

- Standards save cognitive load, for spending elsewhere (direct-to-payload)  

- dbt standardizes the data transformation workflow, overcoming challenges
  - [historically](https://www.getdbt.com/blog/it-s-time-for-open-source-analytics/) 
  - currently

---

# dbt Standardizes Tooling, and also Philosophy

- dbt Core tool is incredible
  - `dbt build` -- 'nuff said.

- dbt blog shares the "real secrets" (principles, "philosophy"):
  - How to clearly structure a complex data project
  - How to unlock tool's full potential -- should be required reading!
  
- In my Data Science experience:
  - I will compose data transformations (that's [good](https://multithreaded.stitchfix.com/blog/2016/03/16/engineers-shouldnt-write-etl/))
  - Non-standard workflows are hard to maintain and extend
      - Does design enforce, modular & orthogonal components? 
      - For new reader, or your future self, what's time-to-understanding (_Art of Readable Code_)?

---

class: inverse, center, middle

# dbt philosophy and tools sharpen data science practice.

---

# Philosophy: Standard Workflow Layers, Details 

- Philosophy articulates,
  - Transformation workflow as standard _layers_
  - Overarching conventions for implementation, style

- Reminds of [normconf](https://normconf.com/) content

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Gonna start a conference called <a href="https://twitter.com/hashtag/NormIPS?src=hash&amp;ref_src=twsrc%5Etfw">#NormIPS</a> that’s just presentations of middlebrow ML topics. “how to structure Python packages 2022”, “how many k-folds is too many”, “how to make the browser pop-up come up when the notebook is done running”, “putting features in Postgres”, etc.</p>&mdash; Vicki (@vboykis) <a href="https://twitter.com/vboykis/status/1552066833582276610?ref_src=twsrc%5Etfw">July 26, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

- Sources: [style guide](https://github.com/dbt-labs/corp/blob/main/dbt_style_guide.md), [best practices document](https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview)

---

# Standard Layers Structure a Workflow

- **"Stacking our transformation in optimized, modular layers means
we can apply each transformation in only one place"**
  - Remarkable -- contrast to decision burden in adhoc approaches

- Staging: atomic building blocks

- Intermediate: transformation steps

- Marts: business-meaningful entities

- Probably not your workflow version 1
  - "With a functioning model flowing in dbt, 
  we can start refactoring and optimizing that mart."
  - Add this complexity as necessary!

---

# A Few Properties Tell a Layer's Story

- Folder structure
  - "Folder structure is also one of the key interfaces
  for understanding the knowledge graph encoded in our project [...]
  It should reflect how the data flows, step-by-step,
  from a wide variety of source-conformed models into 
  fewer, richer business-conformed models."

- File names
  - "By descriptively labeling the transformations happening [within model ...],
  even a stakeholder who doesn't know SQL would be able to grasp 
  the purpose of this section"

- Typical operations
  - Manage complexity by breaking out small steps

---

# Staging Layer: Atomic Building Blocks

- Folder structure
  - One subdirectory per source system (Stripe, transactions database, etc)

- File names
  - Layer, source system, entity (plural): `stg_[source]__[entity]s.sql`

- Typical operations
  - Rename
  - Change data types
  - Univariate transforms
      - Converting units
      - Discretizing/bucketizing
  - NOT joins (layer not dedicated for integrations)
  - NOT aggregations

---

# Intermediate Layer: Transformation Steps

- Folder structure
  - One subdirectory per business area (finance, marketing, etc)

- File names
  - Layer, entity (plural), transform action: `int_[entity]s_[verb]s.sql`
  - Emphasize business-conformed concept over source system
  - "easy for **anybody** to quickly understand 
  what's happening in that model"

- Typical operations
  - Change of data grain
      - Table pivot
      - Aggregations
      - Joins

---

# Mart Layer: Business-Meaningful Entities

- Folder structure
  - One subdirectory per business area (finance, marketing, etc)
  - Not to encourage, same concept calculated differently per team
      - Good: `tax_revenue`, `revenue`
      - Not good: `finance_revenue`, `marketing_revenue`

- File names
  - Entity: `[entity]s.sql`
  - "for pure marts, there should not be a time dimension [...]
  typically best captured via metrics"

- Typical operations
  - Joins
  - Wide array of calculations -- though limited count
      - Many calculations imply, opportunity for upstream move

---

# Miscellaneous insights

- Development Flow

- "It's easy for anybody to quickly understand what's happening in that model, 
even if they don't know SQL"

- Don't over-optimize too early!

---

# Standardized Table/Model Naming

- `<dag_stage>`_`<source/topic>`_`<additional_context>`
  - snake_case
  - plural objects
  
- Name with the onboarding team member or business stakeholder in mind!
  - Limit abbreviations
  - Language that's business-meaningful, not data source jargon
  - _Art of Readable Code_ -- time-to-understanding as key measure of name quality
  
- Booleans prefix with `is_` or `has_`

---

# Code Readability is a Primary Objective

- Do not optimize query for fewer lines of code
  - _Art of Readable Code_
  
- Leverage CTE early and often
  - Import-type
  - Logical-type
  
---

# Why Structure Matters?

---

# Test-Driven Data Transformation

- At minimum, unique and not-null tests necessary for every model
  - Drives immediately quality assurance/improvement in DS work
  - Might even write this test first -- again, whether in dbt setup
  or Jupyter nb

---

class: inverse, center, middle

# `dbt` tooling, to this data scientist

---

# Key dimensions

- Well-encapsulated API
  - dbt build
    - Aligned with _Unit Testing_ -- single command accomplishes objective
    
- Low-visual noise interfaces for configuration
  - yaml files
  
- dbt docs
  
---

# Tooling Becomes Inspiration for Other Work

- Configs for data visualization (snapshot)
- Configs for model-flow (snapshot, link)

---

# Product Concept Where dbt Plays Integral Role